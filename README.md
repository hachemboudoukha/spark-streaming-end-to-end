# Projet Spark Streaming - Analyse de Donn√©es en Temps R√©el

Projet dans le cadre du cours spark de streaming a l'esgi avec **Apache Spark**, **Kafka**, **PostgreSQL** avec **Scala** pour analyser un dataset d'addiction au t√©l√©phone chez les adolescents.

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CSV File      ‚îÇ
‚îÇ  (Dataset)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Python Script  ‚îÇ  ‚Üê Cr√©e les tables et importe CSV
‚îÇ csv_to_postgres ‚îÇ     vers PostgreSQL
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PostgreSQL     ‚îÇ  ‚Üê Base de donn√©es
‚îÇ   (Database)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Producer      ‚îÇ  ‚Üê Lit depuis PostgreSQL
‚îÇ   (Scala)       ‚îÇ     et envoie √† Kafka
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Kafka       ‚îÇ  ‚Üê Message Broker
‚îÇ   (Topic)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Consumer      ‚îÇ  ‚Üê Re√ßoit les donn√©es
‚îÇ Spark Streaming ‚îÇ     et les sauvegarde
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Output CSV    ‚îÇ  ‚Üê R√©sultats sauvegard√©s
‚îÇ  (Processed)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants

1. **Python Script** : Cr√©e les tables PostgreSQL et importe le CSV
2. **PostgreSQL** : Base de donn√©es qui stocke le dataset .sql
3. **Producer** : Lit depuis PostgreSQL et envoie chaque ligne √† Kafka
4. **Kafka** : Message broker qui stocke temporairement les messages
5. **Consumer** : Re√ßoit les donn√©es de Kafka et les sauvegarde
6. **Output** : R√©sultats sauvegard√©s dans `output/processed_data/`

---

## üìã Pr√©requis

Avant de commencer, installez :

- ‚úÖ **Java 11+** (pour Spark)
- ‚úÖ **Docker Desktop** (pour Kafka et PostgreSQL)
- ‚úÖ **IntelliJ IDEA** (avec plugin Scala)
- ‚úÖ **Python 3.8+** (pour le script d'import)
- ‚úÖ **sbt** (gestionnaire de d√©pendances Scala)

---

## üöÄ Installation et Configuration

### √âtape 1 : V√©rifier les pr√©requis

```bash
# V√©rifier Java
java -version

# V√©rifier Docker
docker --version

# V√©rifier Python
python --version
```

### √âtape 2 : D√©marrer les services (Kafka, Zookeeper, PostgreSQL)

Ouvrez un terminal dans le dossier du projet :

```bash
# D√©marrer tous les services
docker-compose up -d
```

**Attendez 20-30 secondes** que tous les services d√©marrent.

V√©rifier que tout est pr√™t :
```bash
docker-compose ps
```

Vous devriez voir :
- `spark_streaming-zookeeper-1` ‚Üí Up
- `spark_streaming-kafka-1` ‚Üí Up
- `spark_streaming-postgres-1` ‚Üí Up

### √âtape 3 : Importer le CSV vers PostgreSQL

Installer les d√©pendances Python :
```bash
pip install -r traitement/requirements.txt
```

Importer le CSV :
```bash
python traitement/csv_to_postgresql.py \
  --csv data/teen_phone_addiction_dataset.csv \
  --password postgres
```

**Options disponibles** :
- `--host` : Host PostgreSQL (d√©faut: localhost)
- `--port` : Port PostgreSQL (d√©faut: 5432)
- `--database` : Nom de la base (d√©faut: spark_streaming)
- `--user` : Utilisateur (d√©faut: postgres)
- `--password` : Mot de passe (requis)

Le script va :
1. ‚úÖ Lire le fichier CSV
2. ‚úÖ Cr√©er la table `teen_phone_addiction` dans PostgreSQL
3. ‚úÖ Importer toutes les donn√©es

---

## ‚ñ∂Ô∏è Ex√©cution de l'Application

### √âtape 1 : Lancer le Consumer (d'abord !)

Dans **IntelliJ IDEA** :

1. Ouvrez le fichier : `src/main/scala/Consumer.scala`
2. Clic droit sur `object Consumer`
3. S√©lectionnez **Run 'Consumer'**

Vous devriez voir :
```
=== CONSUMER D√âMARR√â ===
Connexion √† Kafka...
‚úÖ Consumer pr√™t. Appuyez sur Ctrl+C pour arr√™ter.
```

**‚ö†Ô∏è Important** : Laissez le Consumer en cours d'ex√©cution (ne fermez pas la console).

### √âtape 2 : Lancer le Producer

Dans **IntelliJ IDEA** (nouvelle fen√™tre ou onglet) :

1. Ouvrez le fichier : `src/main/scala/Producer.scala`
2. Clic droit sur `object Producer`
3. S√©lectionnez **Run 'Producer'**

Vous verrez :
```
=== PRODUCER D√âMARR√â ===
Base de donn√©es: jdbc:postgresql://localhost:5432/spark_streaming
Topic Kafka: spark-streaming-topic
‚úÖ Connect√© √† PostgreSQL
Lecture des donn√©es...
Envoy√©: 100 lignes
Envoy√©: 200 lignes
...
‚úÖ Total envoy√©: 3000 lignes
```

### √âtape 3 : Observer les R√©sultats

Dans la console du **Consumer**, toutes les 5 secondes, vous verrez :

```
üì¶ Batch re√ßu: 50 messages
√âchantillon des donn√©es:
+-----------------------------------+
|data                               |
+-----------------------------------+
|1,John Doe,16,Male,New York,10th...|
...
‚úÖ Donn√©es sauvegard√©es
```

Les donn√©es sont sauvegard√©es dans `output/processed_data/`

---

## üìÅ Structure des Fichiers

```
spark_streaming/
‚îÇ
‚îú‚îÄ‚îÄ data/                                    # Donn√©es d'entr√©e
‚îÇ   ‚îî‚îÄ‚îÄ teen_phone_addiction_dataset.csv     # Dataset CSV
‚îÇ
‚îú‚îÄ‚îÄ src/main/scala/                         # Code source
‚îÇ   ‚îú‚îÄ‚îÄ Producer.scala                      # Lit PostgreSQL ‚Üí Envoie √† Kafka
‚îÇ   ‚îî‚îÄ‚îÄ Consumer.scala                     # Re√ßoit Kafka ‚Üí Sauvegarde
‚îÇ
‚îú‚îÄ‚îÄ traitement/                             # Scripts Python
‚îÇ   ‚îú‚îÄ‚îÄ csv_to_postgresql.py                # Import CSV ‚Üí PostgreSQL
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt                    # D√©pendances Python
‚îÇ
‚îú‚îÄ‚îÄ output/                                 # R√©sultats (cr√©√© automatiquement)
‚îÇ   ‚îî‚îÄ‚îÄ processed_data/                     # Donn√©es trait√©es
‚îÇ
‚îú‚îÄ‚îÄ build.sbt                               # D√©pendances du projet
‚îú‚îÄ‚îÄ docker-compose.yml                      # Configuration services
‚îî‚îÄ‚îÄ README.md                               # Ce fichier
```

---

## üîç Comprendre le Code

### Producer.scala

**R√¥le** : Lire depuis PostgreSQL et envoyer √† Kafka

```scala
// 1. Connexion √† PostgreSQL
val connection = DriverManager.getConnection(dbUrl, dbUser, dbPassword)

// 2. Lire les donn√©es
val resultSet = statement.executeQuery("SELECT * FROM teen_phone_addiction")

// 3. Pour chaque ligne, construire CSV et envoyer √† Kafka
while (resultSet.next()) {
  val csvLine = values.mkString(",")
  val record = new ProducerRecord[String, String](topic, csvLine)
  producer.send(record)
}
```

### Consumer.scala

**R√¥le** : Recevoir de Kafka et sauvegarder

```scala
// 1. Cr√©er le stream Kafka
val stream = KafkaUtils.createDirectStream(...)

// 2. Traiter chaque batch (toutes les 5 secondes)
stream.foreachRDD { rdd =>
  // Convertir en DataFrame
  val df = lines.toDF("data")
  
  // Afficher et sauvegarder
  df.show(5)
  df.write.csv("output/processed_data")
}
```

### csv_to_postgresql.py

**R√¥le** : Cr√©er les tables et importer le CSV

```python
# 1. Lire le CSV
df = pd.read_csv(csv_file)

# 2. Connexion √† PostgreSQL
conn = psycopg2.connect(...)

# 3. Cr√©er la table
cursor.execute("CREATE TABLE IF NOT EXISTS ...")

# 4. Importer les donn√©es
for row in df.iterrows():
    cursor.execute("INSERT INTO ... VALUES ...")
```

---

## üõ†Ô∏è Commandes Utiles

### G√©rer les services Docker

```bash
# D√©marrer tous les services
docker-compose up -d

# V√©rifier l'√©tat
docker-compose ps

# Voir les logs PostgreSQL
docker-compose logs postgres

# Voir les logs Kafka
docker-compose logs kafka

# Arr√™ter tous les services
docker-compose down
```

### V√©rifier PostgreSQL

```bash
# Se connecter √† PostgreSQL
docker exec -it spark_streaming-postgres-1 psql -U postgres -d spark_streaming

# Dans psql, v√©rifier les donn√©es
SELECT COUNT(*) FROM teen_phone_addiction;
SELECT * FROM teen_phone_addiction LIMIT 5;
\q
```

### Dans IntelliJ

- **Run** : Clic droit ‚Üí Run
- **Stop** : Bouton rouge dans la console
- **Voir les logs** : Console en bas de l'√©cran

---

## ‚ùì D√©pannage

### Erreur : "Connection refused" (PostgreSQL)

**Cause** : PostgreSQL n'est pas d√©marr√©

**Solution** :
```bash
docker-compose up -d postgres
# Attendre 10 secondes
```

### Erreur : "Table does not exist"

**Cause** : Le CSV n'a pas √©t√© import√©

**Solution** :
```bash
python traitement/csv_to_postgresql.py \
  --csv data/teen_phone_addiction_dataset.csv \
  --password postgres
```

### Erreur : "Connection refused" (Kafka)

**Cause** : Kafka n'est pas d√©marr√©

**Solution** :
```bash
docker-compose up -d kafka zookeeper
# Attendre 15 secondes
```

### Le Consumer ne re√ßoit rien

**Cause** : Le Producer n'a pas √©t√© lanc√© ou a fini

**Solution** : 
1. V√©rifiez que le Producer est en cours d'ex√©cution
2. Relancez le Producer

### Erreur de compilation dans IntelliJ

**Solution** :
1. **File** ‚Üí **Invalidate Caches / Restart**
2. **View** ‚Üí **Tool Windows** ‚Üí **sbt** ‚Üí **Reload sbt project**

---

## üìä Dataset

Le dataset `teen_phone_addiction_dataset.csv` contient :
- **3000+ enregistrements** d'adolescents
- **25 colonnes** : ID, Name, Age, Gender, Daily Usage Hours, Addiction Level, etc.

**Colonnes importantes** :
- `Daily_Usage_Hours` : Heures d'utilisation quotidienne
- `Addiction_Level` : Niveau d'addiction (0-10)
- `Academic_Performance` : Performance acad√©mique (0-100)
- `Gender` : Genre (Male, Female, Other)

---

## üìù Notes Importantes

- ‚ö†Ô∏è **Lancer le Consumer AVANT le Producer**
- ‚ö†Ô∏è **Tous les services Docker doivent √™tre d√©marr√©s avant de lancer l'application**
- ‚ö†Ô∏è **Le CSV doit √™tre import√© dans PostgreSQL avant de lancer le Producer**
- ‚ö†Ô∏è **Le Consumer traite les donn√©es par batch de 5 secondes**
- ‚úÖ Les r√©sultats sont sauvegard√©s automatiquement dans `output/`

---

## üöÄ Prochaines √âtapes

Pour aller plus loin :

1. **Modifier le Consumer** : Ajouter des analyses dans `Consumer.scala`
2. **Changer l'intervalle de batch** : `Seconds(5)` ‚Üí `Seconds(10)`
3. **Ajouter des filtres** : Filtrer certaines donn√©es avant sauvegarde
4. **Exporter vers d'autres bases** : Modifier le Consumer pour √©crire dans PostgreSQL

---

## üìû Support

Si vous rencontrez des probl√®mes :

1. V√©rifiez que tous les services sont d√©marr√©s : `docker-compose ps`
2. V√©rifiez les logs : `docker-compose logs`
3. V√©rifiez que le CSV a √©t√© import√© : Se connecter √† PostgreSQL et v√©rifier

---

**Bon streaming ! üéâ**
