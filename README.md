<<<<<<< HEAD
# Projet Spark Streaming - Analyse de DonnÃ©es en Temps RÃ©el

Projet dans le cadre du cours spark de streaming a l'esgi avec **Apache Spark**, **Kafka**, **PostgreSQL** avec **Scala** pour analyser un dataset d'addiction au tÃ©lÃ©phone chez les adolescents.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV File      â”‚
â”‚  (Dataset)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Script  â”‚  â† CrÃ©e les tables et importe CSV
â”‚ csv_to_postgres â”‚     vers PostgreSQL
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL     â”‚  â† Base de donnÃ©es
â”‚   (Database)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer      â”‚  â† Lit depuis PostgreSQL
â”‚   (Scala)       â”‚     et envoie Ã  Kafka
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Kafka       â”‚  â† Message Broker
â”‚   (Topic)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Consumer      â”‚  â† ReÃ§oit les donnÃ©es
â”‚ Spark Streaming â”‚     et les sauvegarde
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Output CSV    â”‚  â† RÃ©sultats sauvegardÃ©s
â”‚  (Processed)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants

1. **Python Script** : CrÃ©e les tables PostgreSQL et importe le CSV
2. **PostgreSQL** : Base de donnÃ©es qui stocke le dataset .sql
3. **Producer** : Lit depuis PostgreSQL et envoie chaque ligne Ã  Kafka
4. **Kafka** : Message broker qui stocke temporairement les messages
5. **Consumer** : ReÃ§oit les donnÃ©es de Kafka et les sauvegarde
6. **Output** : RÃ©sultats sauvegardÃ©s dans `output/processed_data/`

---

## ðŸ“‹ PrÃ©requis

Avant de commencer, installez :

- âœ… **Java 11+** (pour Spark)
- âœ… **Docker Desktop** (pour Kafka et PostgreSQL)
- âœ… **IntelliJ IDEA** (avec plugin Scala)
- âœ… **Python 3.8+** (pour le script d'import)
- âœ… **sbt** (gestionnaire de dÃ©pendances Scala)

---

## ðŸš€ Installation et Configuration

### Ã‰tape 1 : VÃ©rifier les prÃ©requis

```bash
# VÃ©rifier Java
java -version

# VÃ©rifier Docker
docker --version

# VÃ©rifier Python
python --version
```

### Ã‰tape 2 : DÃ©marrer les services (Kafka, Zookeeper, PostgreSQL)

Ouvrez un terminal dans le dossier du projet :

```bash
# DÃ©marrer tous les services
docker-compose up -d
```

**Attendez 20-30 secondes** que tous les services dÃ©marrent.

VÃ©rifier que tout est prÃªt :
```bash
docker-compose ps
```

Vous devriez voir :
- `spark_streaming-zookeeper-1` â†’ Up
- `spark_streaming-kafka-1` â†’ Up
- `spark_streaming-postgres-1` â†’ Up

### Ã‰tape 3 : Importer le CSV vers PostgreSQL

Installer les dÃ©pendances Python :
```bash
pip install -r traitement/requirements.txt
```

Importer le CSV :
```bash
python traitement/csv_to_postgresql.py \
  --csv data/teen_phone_addiction_dataset.csv \
  --password postgres
```

**Options disponibles** :
- `--host` : Host PostgreSQL (dÃ©faut: localhost)
- `--port` : Port PostgreSQL (dÃ©faut: 5432)
- `--database` : Nom de la base (dÃ©faut: spark_streaming)
- `--user` : Utilisateur (dÃ©faut: postgres)
- `--password` : Mot de passe (requis)

Le script va :
1. âœ… Lire le fichier CSV
2. âœ… CrÃ©er la table `teen_phone_addiction` dans PostgreSQL
3. âœ… Importer toutes les donnÃ©es

---

## â–¶ï¸ ExÃ©cution de l'Application

### Ã‰tape 1 : Lancer le Consumer (d'abord !)

Dans **IntelliJ IDEA** :

1. Ouvrez le fichier : `src/main/scala/Consumer.scala`
2. Clic droit sur `object Consumer`
3. SÃ©lectionnez **Run 'Consumer'**

Vous devriez voir :
```
=== CONSUMER DÃ‰MARRÃ‰ ===
Connexion Ã  Kafka...
âœ… Consumer prÃªt. Appuyez sur Ctrl+C pour arrÃªter.
```

**âš ï¸ Important** : Laissez le Consumer en cours d'exÃ©cution (ne fermez pas la console).

### Ã‰tape 2 : Lancer le Producer

Dans **IntelliJ IDEA** (nouvelle fenÃªtre ou onglet) :

1. Ouvrez le fichier : `src/main/scala/Producer.scala`
2. Clic droit sur `object Producer`
3. SÃ©lectionnez **Run 'Producer'**

Vous verrez :
```
=== PRODUCER DÃ‰MARRÃ‰ ===
Base de donnÃ©es: jdbc:postgresql://localhost:5432/spark_streaming
Topic Kafka: spark-streaming-topic
âœ… ConnectÃ© Ã  PostgreSQL
Lecture des donnÃ©es...
EnvoyÃ©: 100 lignes
EnvoyÃ©: 200 lignes
...
âœ… Total envoyÃ©: 3000 lignes
```

### Ã‰tape 3 : Observer les RÃ©sultats

Dans la console du **Consumer**, toutes les 5 secondes, vous verrez :

```
ðŸ“¦ Batch reÃ§u: 50 messages
Ã‰chantillon des donnÃ©es:
+-----------------------------------+
|data                               |
+-----------------------------------+
|1,John Doe,16,Male,New York,10th...|
...
âœ… DonnÃ©es sauvegardÃ©es
```

Les donnÃ©es sont sauvegardÃ©es dans `output/processed_data/`

---

## ðŸ“ Structure des Fichiers

```
spark_streaming/
â”‚
â”œâ”€â”€ data/                                    # DonnÃ©es d'entrÃ©e
â”‚   â””â”€â”€ teen_phone_addiction_dataset.csv     # Dataset CSV
â”‚
â”œâ”€â”€ src/main/scala/                         # Code source
â”‚   â”œâ”€â”€ Producer.scala                      # Lit PostgreSQL â†’ Envoie Ã  Kafka
â”‚   â””â”€â”€ Consumer.scala                     # ReÃ§oit Kafka â†’ Sauvegarde
â”‚
â”œâ”€â”€ traitement/                             # Scripts Python
â”‚   â”œâ”€â”€ csv_to_postgresql.py                # Import CSV â†’ PostgreSQL
â”‚   â””â”€â”€ requirements.txt                    # DÃ©pendances Python
â”‚
â”œâ”€â”€ output/                                 # RÃ©sultats (crÃ©Ã© automatiquement)
â”‚   â””â”€â”€ processed_data/                     # DonnÃ©es traitÃ©es
â”‚
â”œâ”€â”€ build.sbt                               # DÃ©pendances du projet
â”œâ”€â”€ docker-compose.yml                      # Configuration services
â””â”€â”€ README.md                               # Ce fichier
```

---

## ðŸ” Comprendre le Code

### Producer.scala

**RÃ´le** : Lire depuis PostgreSQL et envoyer Ã  Kafka

```scala
// 1. Connexion Ã  PostgreSQL
val connection = DriverManager.getConnection(dbUrl, dbUser, dbPassword)

// 2. Lire les donnÃ©es
val resultSet = statement.executeQuery("SELECT * FROM teen_phone_addiction")

// 3. Pour chaque ligne, construire CSV et envoyer Ã  Kafka
while (resultSet.next()) {
  val csvLine = values.mkString(",")
  val record = new ProducerRecord[String, String](topic, csvLine)
  producer.send(record)
}
```

### Consumer.scala

**RÃ´le** : Recevoir de Kafka et sauvegarder

```scala
// 1. CrÃ©er le stream Kafka
val stream = KafkaUtils.createDirectStream(...)

// 2. Traiter chaque batch (toutes les 5 secondes)
stream.foreachRDD { rdd =>
  // Convertir en DataFrame
  val df = lines.toDF("data")
  
  // Afficher et sauvegarder
  df.show(5)
  df.write.csv("output/processed_data")
}
```

### csv_to_postgresql.py

**RÃ´le** : CrÃ©er les tables et importer le CSV

```python
# 1. Lire le CSV
df = pd.read_csv(csv_file)

# 2. Connexion Ã  PostgreSQL
conn = psycopg2.connect(...)

# 3. CrÃ©er la table
cursor.execute("CREATE TABLE IF NOT EXISTS ...")

# 4. Importer les donnÃ©es
for row in df.iterrows():
    cursor.execute("INSERT INTO ... VALUES ...")
```

---

## ðŸ› ï¸ Commandes Utiles

### GÃ©rer les services Docker

```bash
# DÃ©marrer tous les services
docker-compose up -d

# VÃ©rifier l'Ã©tat
docker-compose ps

# Voir les logs PostgreSQL
docker-compose logs postgres

# Voir les logs Kafka
docker-compose logs kafka

# ArrÃªter tous les services
docker-compose down
```

### VÃ©rifier PostgreSQL

```bash
# Se connecter Ã  PostgreSQL
docker exec -it spark_streaming-postgres-1 psql -U postgres -d spark_streaming

# Dans psql, vÃ©rifier les donnÃ©es
SELECT COUNT(*) FROM teen_phone_addiction;
SELECT * FROM teen_phone_addiction LIMIT 5;
\q
```

### Dans IntelliJ

- **Run** : Clic droit â†’ Run
- **Stop** : Bouton rouge dans la console
- **Voir les logs** : Console en bas de l'Ã©cran

---

## â“ DÃ©pannage

### Erreur : "Connection refused" (PostgreSQL)

**Cause** : PostgreSQL n'est pas dÃ©marrÃ©

**Solution** :
```bash
docker-compose up -d postgres
# Attendre 10 secondes
```

### Erreur : "Table does not exist"

**Cause** : Le CSV n'a pas Ã©tÃ© importÃ©

**Solution** :
```bash
python traitement/csv_to_postgresql.py \
  --csv data/teen_phone_addiction_dataset.csv \
  --password postgres
```

### Erreur : "Connection refused" (Kafka)

**Cause** : Kafka n'est pas dÃ©marrÃ©

**Solution** :
```bash
docker-compose up -d kafka zookeeper
# Attendre 15 secondes
```

### Le Consumer ne reÃ§oit rien

**Cause** : Le Producer n'a pas Ã©tÃ© lancÃ© ou a fini

**Solution** : 
1. VÃ©rifiez que le Producer est en cours d'exÃ©cution
2. Relancez le Producer

### Erreur de compilation dans IntelliJ

**Solution** :
1. **File** â†’ **Invalidate Caches / Restart**
2. **View** â†’ **Tool Windows** â†’ **sbt** â†’ **Reload sbt project**

---

## ðŸ“Š Dataset

Le dataset `teen_phone_addiction_dataset.csv` contient :
- **3000+ enregistrements** d'adolescents
- **25 colonnes** : ID, Name, Age, Gender, Daily Usage Hours, Addiction Level, etc.

**Colonnes importantes** :
- `Daily_Usage_Hours` : Heures d'utilisation quotidienne
- `Addiction_Level` : Niveau d'addiction (0-10)
- `Academic_Performance` : Performance acadÃ©mique (0-100)
- `Gender` : Genre (Male, Female, Other)

---

## ðŸ“ Notes Importantes

- âš ï¸ **Lancer le Consumer AVANT le Producer**
- âš ï¸ **Tous les services Docker doivent Ãªtre dÃ©marrÃ©s avant de lancer l'application**
- âš ï¸ **Le CSV doit Ãªtre importÃ© dans PostgreSQL avant de lancer le Producer**
- âš ï¸ **Le Consumer traite les donnÃ©es par batch de 5 secondes**
- âœ… Les rÃ©sultats sont sauvegardÃ©s automatiquement dans `output/`

---

## ðŸš€ Prochaines Ã‰tapes

Pour aller plus loin :

1. **Modifier le Consumer** : Ajouter des analyses dans `Consumer.scala`
2. **Changer l'intervalle de batch** : `Seconds(5)` â†’ `Seconds(10)`
3. **Ajouter des filtres** : Filtrer certaines donnÃ©es avant sauvegarde
4. **Exporter vers d'autres bases** : Modifier le Consumer pour Ã©crire dans PostgreSQL

---

## ðŸ“ž Support

Si vous rencontrez des problÃ¨mes :

1. VÃ©rifiez que tous les services sont dÃ©marrÃ©s : `docker-compose ps`
2. VÃ©rifiez les logs : `docker-compose logs`
3. VÃ©rifiez que le CSV a Ã©tÃ© importÃ© : Se connecter Ã  PostgreSQL et vÃ©rifier

---

**Bon streaming ! ðŸŽ‰**
=======
# spark-streming
end to end pipeline data "teen_phone_addiction"
>>>>>>> 5795b1146a2d9dc38de5fd90d071142d72cb1775
